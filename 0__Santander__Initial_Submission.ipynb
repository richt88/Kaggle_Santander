{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santander Value Prediction Challenge\n",
    "\n",
    "Status - *Initial submission, possible improvements to come following some experimentation.*\n",
    "\n",
    "Link to [Kaggle competion](https://www.kaggle.com/c/santander-value-prediction-challenge) \n",
    "\n",
    "#### Includes :\n",
    "1. Feature Engineering\n",
    "    * Additional features created. \n",
    "2. Preprocessing\n",
    "    * Remove features with zero varience.\n",
    "    * Where features are (nearly) perfectly correlated, remove one.\n",
    "    * Apply log transform to features where the normality is improved by doing so.\n",
    "    * MinMax Scale\n",
    "    * Remove least important features\n",
    "3. Ensemble of Gradient Boosting Regressor and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Scikit-Learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "\n",
    "#Stats\n",
    "from scipy.stats import linregress, shapiro\n",
    "\n",
    "#Random Seed (for reproducibility)\n",
    "seed = 42\n",
    "np.random.seed = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Import Required Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Training Data\n",
    "train = pd.read_csv('datasets/train.csv')\n",
    "X = train.drop(['target', 'ID'], axis = 1)\n",
    "y = round(np.log1p(train.target),32) #rounding off unnecessary precision.\n",
    "ID = train.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Competition Test Data\n",
    "comp = pd.read_csv('datasets/test.csv')\n",
    "X_comp = comp.drop(['ID'], axis = 1) \n",
    "ID_comp = comp.ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metric\n",
    "Performance metric defined in the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, pred):\n",
    "    \"\"\"Performance metric used in kaggle competition\"\"\"\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(pred), 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "###   Feature Engineering \n",
    "Since all feature names are hashed it is not possible to test logical combinations for new features.   \n",
    "However, all features seem to be monetary values. Therefore, various metrics of all these will be created.   \n",
    "This will not harm prediction as features with minimal importance are removed later.\n",
    "\n",
    "For `feature sum` and `sum_pos_neg_correlations` the direction of correlation for each feature is taken into account for the calculations. This is to cover the fact that some feature values could have a negative impact on the `target` i.e. debts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Additional_Features(TransformerMixin):\n",
    "    \"\"\"Additional predictor features created.\n",
    "    \n",
    "    feature_sum: Sum of all features factored by -1 or 1 \n",
    "                 depending on that features correlation with the target\n",
    "    median: Median of all fetures.\n",
    "    max: Max of all features.\n",
    "    var: Varience of values across all features.\n",
    "    N_Not_0: Number of non zero features.\"\"\"\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        self.slope_dir = pd.DataFrame()\n",
    "        self.cols = []\n",
    "    \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        print(\"Fitting Additional_Features\")\n",
    "        #For each feature the +ve or -ve correlation with the target is calculated\n",
    "        self.cols = X.select_dtypes(include = 'number').columns\n",
    "        self.slope_dir = pd.DataFrame(index=self.cols,\n",
    "                          data = {'slope_dir': 0})                \n",
    "\n",
    "        \n",
    "        for col in self.cols:\n",
    "            rows = X[col] != 0\n",
    "            valid_rows = X[rows]\n",
    "            \n",
    "            # Unlikely to get reliable correlation from < 5 data points.\n",
    "            if len(valid_rows) < 5 :\n",
    "                continue\n",
    "                \n",
    "            # Check direction of correlation with target.\n",
    "            # Target data already log_1p transformed, assumption is that log_1p(x) will give better linear fit.\n",
    "            log_x = np.log1p(valid_rows.loc[:,col])\n",
    "            LR = linregress(log_x, y[rows])\n",
    "            if np.logical_or(np.isnan(LR.slope), np.isnan(LR.pvalue)):\n",
    "                continue\n",
    "            else:\n",
    "                # Where p value of slope is < 0.1 correlation is taken as 0.\n",
    "                self.slope_dir.loc[col,'slope_dir'] = (LR.slope * (LR.pvalue < 0.1)) / abs(LR.slope)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **trans_params):\n",
    "        print(\"Transforming Additional_Features\")\n",
    "        X_out = X.copy()\n",
    "        #Sum of all features factored by their +/- correlation with target\n",
    "        X_out['feature_sum'] = X.loc[:,self.cols].apply(lambda x: (x * self.slope_dir.loc[self.cols, 'slope_dir']).sum(), axis = 1)\n",
    "        X_out['sum_pos_neg_correlations'] =  X.loc[:,self.cols].apply(lambda x: ((x != 0) * self.slope_dir.loc[self.cols, 'slope_dir']).sum(), axis = 1)\n",
    "        #Further Fetures\n",
    "        X_out['mean'] = X[self.cols].apply(lambda x: np.mean(x[x != 0]), axis = 1)        \n",
    "        X_out['median'] = X[self.cols].apply(lambda x: np.median(x[x != 0]), axis = 1)\n",
    "        X_out['max'] = X[self.cols].apply(lambda x: np.max(x[x != 0]), axis = 1)\n",
    "        X_out['min'] = X[self.cols].apply(lambda x: np.min(x[x != 0]), axis = 1)        \n",
    "        X_out['var'] = X[self.cols].apply(lambda x: np.var(x[x != 0]), axis = 1)\n",
    "        X_out['N_Not_0'] = X[self.cols].apply(lambda x: (x != 0).sum(), axis = 1)\n",
    "        \n",
    "        \n",
    "        # Remove NaN\n",
    "        X_out[np.isnan(X_out)] = 0\n",
    "                \n",
    "        return X_out  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions\n",
    "To maintain a clean workflow (making use of a pipeline) all preprocessing is completed within custom transformer classes.  \n",
    "All preprocessing is done within a single pipeline, outputing a pandas dataframe. Predictive modelling is completed separate to this, to remove the need to repeat the preprocessing while experimenting. \n",
    "\n",
    "Note that although `MinMaxScaler` is already compatible with pipelines, a custom version is created here to output a pandas dataframe rather than a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------Remove_Zero_Var---------------------------------------------#  \n",
    "class Remove_Zero_Var(TransformerMixin):\n",
    "    \"\"\"Removes features with single values i.e. zero varience\"\"\"\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        self.cols_with_0_var = []\n",
    "        pass\n",
    "            \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        print(\"Fitting Remove_Zero_Var\")\n",
    "        self.cols_with_0_var = X.columns[X.nunique() == 1]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **trans_params):\n",
    "        print(\"Transforming Remove_Zero_Var\")\n",
    "        X_out = X.drop(self.cols_with_0_var, axis=1)\n",
    "        return X_out\n",
    "    \n",
    "#--------------------------Remove_Correlated----------------------------------------------#\n",
    "# Features with perfect correlation are hold redundant information.\n",
    "# currently set to remove features where another has a correlation coefficient of >=0.99.\n",
    "class Remove_Correlated(TransformerMixin):\n",
    "    \"\"\"Removes columns where another column is perfectly (>0.99) correlated.\"\"\"\n",
    "    def __init__(self, **params):\n",
    "        self.redun_cols = []\n",
    "        self.correlated_data = []\n",
    "        self.num_cols = []\n",
    "        pass\n",
    "           \n",
    "        \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        print(\"Fitting Remove_Correlated\")\n",
    "        self.num_cols = X.select_dtypes(include = 'number').columns\n",
    "        \n",
    "        for i, col in enumerate(self.num_cols):\n",
    "           \n",
    "            rows = (X[col]!=0)\n",
    "            \n",
    "            # Unlikely to get reliable correlation from < 5 data points.\n",
    "            # Features with < 5 values will likely be removed later by Remove_Least_Important.\n",
    "            if sum(rows) < 5:\n",
    "                continue\n",
    "            \n",
    "            # Feature Data\n",
    "            temp_1 = X.loc[rows, col]\n",
    "            \n",
    "            # Cycle over all other features to check for correlation\n",
    "            for j in np.arange(i + 1, len(self.num_cols)):\n",
    "                temp_2 = X.loc[rows, self.num_cols[j]]\n",
    "                \n",
    "                # No 0's in temp_1 so correlation is unlikely.\n",
    "                if (temp_2 == 0).any():\n",
    "                    continue\n",
    "                # Check correlation\n",
    "                if np.corrcoef(temp_1, temp_2)[0][1] >= 0.99:\n",
    "                    self.redun_cols.append(col)\n",
    "                    self.correlated_data.append((temp_1,temp_2))\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, **trans_params):\n",
    "        print(\"Transforming Remove_Correlated\")\n",
    "        X_out = X.drop(self.redun_cols, axis=1)\n",
    "        return X_out\n",
    "    \n",
    "#----------------------------Remove_Least_Important--------------------------------------------# \n",
    "# Remove features with an importance rating less than the threshold passed.\n",
    "# Based on a random forest regressor.\n",
    "# Threshold set relativly low (10^-5) to ensure minimal loss of useful information.\n",
    "class Remove_Least_Important(TransformerMixin):\n",
    "    \"\"\"Finds and removes least important features.\n",
    "    \n",
    "    Based on a Random Forest Regressor.\n",
    "    threshold: features with an importance rating below this are removed.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.0001, random_seed=42, **params):\n",
    "        self.threshold=threshold\n",
    "        self.unimportant_cols = []\n",
    "        self.importance = pd.DataFrame()\n",
    "        self.random_seed = random_seed\n",
    "            \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        print(\"Fitting Remove_least_important\")\n",
    "        model = RandomForestRegressor(random_state=self.random_seed)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        self.importance = pd.DataFrame({'col':X.columns,\n",
    "                                   'importance':model.feature_importances_})\\\n",
    "                                    .sort_values(by='importance', ascending=False)\n",
    "\n",
    "        self.unimportant_cols = self.importance.loc[self.importance['importance'] < self.threshold, 'col'].values\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, **trans_params):\n",
    "            print(\"Transforming Remove_least_important\")\n",
    "            X_out = X.drop(self.unimportant_cols, axis=1)\n",
    "            return X_out\n",
    "\n",
    "#-------------------------Log_Trans_X-----------------------------------------------#          \n",
    "# For each feature the shapiro normality metric is calculated.\n",
    "# A log tranform is applied if this metric is improved by it, else it is unchanged.\n",
    "# Currently resets any inf values to 0 - to be reviewed.\n",
    "\n",
    "class Log_Trans_X(TransformerMixin):\n",
    "    \"\"\"Apply np.log1p to feature where it's shapiro normality metric improves by doing so.\"\"\"\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        from scipy.stats import shapiro\n",
    "        self.col_to_be_log = []\n",
    "        self.num_cols = []\n",
    "            \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        print(\"Fitting Log_Trans_X\")\n",
    "        self.num_cols = X.select_dtypes(include = 'number').columns\n",
    "        for col in self.num_cols:\n",
    "            temp_orig = X.loc[X[col]!=0, col]\n",
    "            \n",
    "            # Ensure all positive values\n",
    "            if temp_orig.min() < 0:\n",
    "                temp_orig = temp_orig - temp_orig.min()\n",
    "            # Ignore features with minimal data   \n",
    "            if np.logical_or(len(temp_orig)<=3, temp_orig.nunique()==1):\n",
    "                continue            \n",
    "            else:\n",
    "                temp_log = np.log1p(temp_orig)\n",
    "                \n",
    "                temp_orig_shapiro = shapiro(temp_orig)[0]\n",
    "                temp_log_shapiro = shapiro(temp_log)[0]\n",
    "                \n",
    "                if temp_log_shapiro > temp_orig_shapiro:\n",
    "                    self.col_to_be_log.append(col)                \n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **trans_params):\n",
    "            print(\"Transforming Log_Trans_X\")\n",
    "            X_out = X.copy()\n",
    "            for col in self.col_to_be_log:\n",
    "                # Ensure all positive values\n",
    "                if X_out[col].min() < 0:\n",
    "                    X_out[col] = X_out[col] - X_out[col].min() \n",
    "                # Log values\n",
    "                X_out[col] = np.log1p(X_out[col])\n",
    "            # Remove errors\n",
    "            X_out[np.isinf(X_out)] = 0\n",
    "            X_out[np.isnan(X_out)] = 0\n",
    "            return X_out\n",
    "   \n",
    "             \n",
    "#---------------------MinMax_Df---------------------------------------------------#\n",
    "# Custom transform to output a pandas dataframe rather than a numpy array.\n",
    "class MinMax_Df(TransformerMixin):\n",
    "    \"\"\"Custom transformer to apply MinMax scaling and return a dataframe.\"\"\"\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        self.scaler = MinMaxScaler()\n",
    "            \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        print(\"Fitting MinMax_Df\")\n",
    "        self.scaler.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **trans_params):\n",
    "        print(\"Transforming MinMax_Df\")\n",
    "        data = self.scaler.transform(X)\n",
    "        X_out = pd.DataFrame(index = X.index,\n",
    "                            columns = X.columns,\n",
    "                            data = data)\n",
    "        return X_out\n",
    "\n",
    "#------------------------Round------------------------------------------------# \n",
    "# Removal of unnecessary precision.\n",
    "class Round(TransformerMixin):\n",
    "    \"\"\"Rounds all values off to npd decimal places. Further precision is unnecessary.\"\"\"\n",
    "    \n",
    "    def __init__(self, ndp, **params):\n",
    "        self.ndp = ndp\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        print(\"Fitting Round\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **trans_params):\n",
    "        print(\"Transforming Round\")\n",
    "        X_out = X.round(self.ndp)\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipe\n",
    "All feature engineering and preprocessing is organised into one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipe = Pipeline([\n",
    "        ('remove_zero_varience', Remove_Zero_Var()),\n",
    "        ('added_features', Additional_Features()),    \n",
    "        ('log_transform_X', Log_Trans_X()),    \n",
    "        ('minmax_scale', MinMax_Df()),\n",
    "        ('round', Round(32)),\n",
    "        ('remove_least_important', Remove_Least_Important(threshold=10**-5, random_seed=seed)),\n",
    "        ('remove_correlating_feats', Remove_Correlated())\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Remove_Zero_Var\n",
      "Transforming Remove_Zero_Var\n",
      "Fitting Additional_Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rich\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:107: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = r_num / ssxm\n",
      "C:\\Users\\Rich\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_mstats_common.py:119: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  sterrest = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming Additional_Features\n",
      "Fitting Log_Trans_X\n",
      "Transforming Log_Trans_X\n",
      "Fitting MinMax_Df\n",
      "Transforming MinMax_Df\n",
      "Fitting Round\n",
      "Transforming Round\n",
      "Fitting Remove_least_important\n",
      "Transforming Remove_least_important\n",
      "Fitting Remove_Correlated\n",
      "Transforming Remove_Correlated\n"
     ]
    }
   ],
   "source": [
    "X_clean = preprocess_pipe.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean.to_csv('datasets/0__X_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Evaluation\n",
    "Function to apply kaggle performance metric on potential models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_rmsle(model, X, y, n_tests=5, seed=None):\n",
    "    scores = []\n",
    "    \n",
    "    for i in np.arange(n_tests):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = seed + i)\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = np.expm1(model.predict(X_test))\n",
    "        scores.append(rmsle(np.expm1(y_test), pred))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor\n",
    "Sereval iterations of GridSearch completed on a limited number of hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Model\n",
    "#GBR = GradientBoostingRegressor()\n",
    "\n",
    "#### Grid Search\n",
    "# GBR_tuning_params = [{'n_estimators':[30, 50, 70],\n",
    "#                  'max_depth':[6, 9, 12],\n",
    "#                  'learning_rate':[0.03, 0.05, 0.07],\n",
    "#                  'max_features' : [750, 1000, None]\n",
    "#                  }]\n",
    "\n",
    "# GBR_Search = GridSearchCV(GBR, \n",
    "#                       GBR_tuning_params,\n",
    "#                       cv=3, \n",
    "#                       scoring= 'neg_mean_squared_error',\n",
    "#                      verbose = 5,\n",
    "#                          n_jobs = 8)\n",
    "# GBR_Search.fit(X_clean, y)\n",
    "#GBR_Search.best_params_\n",
    "# Results learning rate = 0.05 , max_depth = 9 , n_estimators = 70, max_features = 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuned Model\n",
    "GBR = GradientBoostingRegressor(n_estimators=70, \n",
    "                                learning_rate=0.05,\n",
    "                                max_depth=9,\n",
    "                                max_features=1000,\n",
    "                               random_state = seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3489227848059147\n"
     ]
    }
   ],
   "source": [
    "# Check tuned model with cross validation\n",
    "error_GBR = cv_rmsle(GBR, X_clean, y, seed = seed)\n",
    "print(np.mean(error_GBR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "Sereval iterations of GridSearch completed on a limited number of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFR = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "# ### Grid Search\n",
    "# RFR_tuning_params = [{'n_estimators':[300, 500, 800],\n",
    "#                  'max_depth':[15, 20, 30, None],\n",
    "#                  'max_features' : [500, 1000, 1500]\n",
    "#                  }]\n",
    "\n",
    "# RFR_Search = GridSearchCV(RFR, \n",
    "#                       RFR_tuning_params,\n",
    "#                       cv=3, \n",
    "#                       scoring= 'neg_mean_squared_error',\n",
    "#                       verbose = 3)\n",
    "# RFR_Search.fit(X_clean, y)\n",
    "#RFR_Search.best_params_\n",
    "#Results max_features = 800 , max_depth = 15, n_estimaters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuned Model\n",
    "RFR = RandomForestRegressor(max_features=800,\n",
    "                           max_depth = 15,\n",
    "                           n_estimators = 500,\n",
    "                           random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3370990857851945\n"
     ]
    }
   ],
   "source": [
    "# Check tuned model with cross validation\n",
    "error_RFR = cv_rmsle(RFR, X_clean, y, seed = seed)\n",
    "print(np.mean(error_RFR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble_Model(BaseEstimator):  \n",
    "    \"\"\"Ensemble of regression models\"\"\"\n",
    "\n",
    "    def __init__(self, models, **Params):\n",
    "        self.models = models\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for m in self.models:\n",
    "            m.fit(X,y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        \n",
    "        for m in self.models:\n",
    "            indv_model = np.expm1(np.array(m.predict(X)))\n",
    "            y_pred += indv_model\n",
    "            \n",
    "        y_pred = np.log1p((list(y_pred/len(self.models))))   \n",
    "            \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3377839173717652\n"
     ]
    }
   ],
   "source": [
    "Ensemble = Ensemble_Model(models=[GBR, RFR])\n",
    "error_ensemble = (cv_rmsle(Ensemble, X_clean, y, seed = seed))\n",
    "print(np.mean(error_ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming Remove_Zero_Var\n",
      "Transforming Additional_Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rich\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Rich\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming Log_Trans_X\n",
      "Transforming MinMax_Df\n",
      "Transforming Round\n",
      "Transforming Remove_least_important\n",
      "Transforming Remove_Correlated\n"
     ]
    }
   ],
   "source": [
    "# Use pre-processing pipeline on competition data\n",
    "X_comp_clean = preprocess_pipe.transform(X_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comp_clean.to_csv('datasets/0__X_comp_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all data\n",
    "Ensemble_comp = Ensemble_Model(models = [RFR, GBR])\n",
    "Ensemble_comp.fit(X_clean, y)\n",
    "y_comp = np.expm1(Ensemble_comp.predict(X_comp_clean))\n",
    "\n",
    "\n",
    "comp_sumbmission = pd.DataFrame({'ID' : ID_comp,\n",
    "                                'target' : y_comp})\n",
    "comp_sumbmission.to_csv('0__Submission.csv',\n",
    "                       index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Score - 1.40"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
